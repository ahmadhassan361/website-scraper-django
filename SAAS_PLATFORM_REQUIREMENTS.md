# ScrapeMaster Pro - Complete SaaS Platform Requirements

## üìã Project Overview

**ScrapeMaster Pro** is a comprehensive web scraping SaaS platform that enables users to effortlessly extract data from Shopify and custom websites. Built with Django and Celery, the platform offers automated scraping, scheduling capabilities, multi-format exports (CSV, Excel, Google Sheets), and a robust multi-tenant architecture with subscription-based access.

### üéØ Core Value Proposition
- **Automated Data Extraction**: Set up once, scrape automatically on schedule
- **Multi-Platform Support**: Shopify stores + custom websites with team-developed scrapers
- **Flexible Export Options**: Export to CSV, Excel, Google Sheets with custom field selection
- **Scalable Plans**: From free tier to enterprise solutions
- **Manual Payment Integration**: Supporting multiple payment methods for global accessibility

### üöÄ Target Audience
- E-commerce businesses monitoring competitors
- Market researchers tracking product data
- Data analysts requiring automated data collection
- Small to medium businesses needing competitive intelligence

---

## üèóÔ∏è Technical Architecture

### Current Foundation
- **Backend**: Django 5.2+ with Celery for background processing
- **Database**: PostgreSQL with multi-tenant data isolation
- **Queue System**: Redis/Celery for scraping tasks
- **Authentication**: Custom + Google OAuth2
- **Export Integration**: Google Drive/Sheets API with OAuth2
- **Frontend**: Bootstrap-based responsive UI

### Planned Enhancements
- **UI Framework**: Modern dark theme with violet/blue/green color scheme
- **Payment**: Manual verification system for global accessibility
- **Admin Panel**: Comprehensive super admin dashboard
- **Email System**: Automated templates for all user communications
- **Dynamic Scrapers**: Database-stored code execution for custom sites

---

## üìä Feature Modules & Requirements

## Module 1: UI/UX Redesign üé®

### Objectives
Transform the current interface into a modern, professional SaaS platform with intuitive navigation and beautiful aesthetics.

### Requirements
- **Color Scheme**: Dark violet primary, sea green accents, blue highlights
- **Design Style**: Modern, clean, minimalist with proper spacing
- **Components**: 
  - Dashboard with real-time metrics
  - User-friendly forms with validation
  - Responsive data tables
  - Progress indicators and notifications
  - Mobile-responsive design
- **Navigation**: Intuitive sidebar with collapsible menu
- **Branding**: Professional logo, consistent typography

### Expected Output
- Modern dashboard that rivals industry leaders (Scrapy Cloud, Apify)
- Improved user experience with reduced learning curve
- Mobile-friendly interface for monitoring on-the-go

### Implementation Approach
- Implement custom CSS framework with CSS variables for theming
- Use Bootstrap 5 as foundation with custom overrides
- Implement dark/light mode toggle
- Add loading animations and micro-interactions

---

## Module 2: Multi-Tenant Authentication System üîê

### Objectives
Create a secure, scalable user management system with multiple authentication methods and complete data isolation.

### Requirements

#### Authentication Methods
- **Google OAuth2**: One-click registration/login
- **Email/Password**: Traditional signup with email verification
- **OTP Verification**: SMS/Email-based two-factor authentication

#### User Management
- **Registration Flow**: Email verification ‚Üí Plan selection ‚Üí Account setup
- **Profile Management**: Avatar, contact details, preferences
- **Password Security**: Strong password requirements, reset functionality
- **Session Management**: Secure sessions with configurable timeout

#### Data Isolation
- **Complete Separation**: Users see only their own data (websites, products, sessions)
- **Secure Queries**: All database queries filtered by user ownership
- **Admin Override**: Super admin can view all data when needed

### Expected Output
- Secure multi-tenant system with zero data leakage
- Smooth onboarding experience for new users
- Professional authentication flow matching enterprise standards

### Implementation Approach
- Extend Django User model with custom profile
- Implement middleware for automatic user filtering
- Use Django's built-in permission system with custom decorators
- Integrate Google OAuth2 with automatic profile creation

---

## Module 3: Subscription Management System üí≥

### Objectives
Implement a flexible, admin-configurable subscription system with usage tracking and manual payment processing.

### Subscription Plans

#### üÜì Free Plan
- **Shopify Sites**: 3 maximum
- **Scraping Hits**: 5 per website per month
- **Custom Sites**: 0
- **Export Formats**: CSV only
- **Support**: Community forum
- **Price**: $0/month

#### ü•â Basic Plan
- **Shopify Sites**: 10 maximum  
- **Scraping Hits**: 15 per website per month
- **Custom Sites**: 0
- **Export Formats**: CSV, Excel
- **Support**: Email support
- **Price**: Admin configurable

#### ü•à Standard Plan  
- **Shopify Sites**: 20 maximum
- **Custom Sites**: 3 requests allowed
- **Scraping Hits**: 30 per Shopify site, 10 per custom site per month
- **Export Formats**: All formats including Google Sheets
- **Support**: Priority email support
- **Price**: Admin configurable

#### ü•á Premium Plan
- **Shopify Sites**: 50 maximum
- **Custom Sites**: 5 requests allowed  
- **Scraping Hits**: 50 per Shopify site, 15 per custom site per month
- **Export Formats**: All formats with advanced features
- **Support**: Priority support + phone calls
- **Price**: Admin configurable

#### üí∞ Add-on Purchases
- **Extra Shopify Site**: $5 per site
- **Extra Custom Site**: $50 per site (includes development)
- **Extra Scraping Hits**: Configurable pricing per hit package

### Payment Methods
- PayPal direct transfer
- Fiverr order completion  
- Payoneer transfer
- Crypto: USDT, Bitcoin, Ethereum
- Bank transfer (for enterprise)

### Payment Process
1. User selects plan and completes signup
2. System generates invoice with payment instructions
3. User makes payment using preferred method
4. User uploads payment proof/transaction ID
5. Admin verifies payment manually
6. Subscription activated automatically upon verification

### Expected Output
- Flexible subscription system adaptable to market changes
- Clear usage tracking and limits enforcement
- Streamlined payment verification workflow
- Automated subscription lifecycle management

### Implementation Approach
- Create subscription models with configurable limits
- Implement usage tracking middleware
- Build payment verification workflow with admin approval
- Add automated email notifications for all subscription events

---

## Module 4: Super Admin Dashboard üëë

### Objectives
Create a comprehensive administrative interface for managing users, subscriptions, custom scraper requests, and system monitoring.

### Requirements

#### User Management
- **User Overview**: List all users with subscription status, usage statistics
- **User Details**: Complete profile, subscription history, payment records
- **User Actions**: Suspend, activate, modify limits, send notifications
- **Support Tools**: View user's data, impersonate for troubleshooting

#### Subscription Management  
- **Plan Configuration**: Set pricing, limits, features for each plan
- **Payment Verification**: Review payment proofs, approve/reject
- **Usage Monitoring**: Track API usage, detect abuse patterns  
- **Revenue Analytics**: Subscription revenue, churn analysis, growth metrics

#### Custom Scraper Management
- **Request Queue**: View all custom site requests with priority levels
- **Development Tracking**: Assign requests to developers, track progress
- **Code Repository**: Manage scraper templates and custom implementations
- **Testing Interface**: Test custom scrapers before deployment

#### System Monitoring
- **Performance Metrics**: Server load, scraping success rates, error tracking
- **Queue Management**: Monitor Celery tasks, failed jobs, system health
- **User Activity**: Login patterns, feature usage, support requests
- **Financial Dashboard**: Revenue tracking, subscription conversions

### Expected Output
- Enterprise-grade admin interface for complete system control
- Efficient workflows for managing custom development requests
- Real-time monitoring and alerting for system issues
- Data-driven insights for business decision making

### Implementation Approach
- Extend Django admin with custom views and dashboards
- Implement role-based access for different admin levels
- Create custom admin templates matching main site theme
- Add automated monitoring with email alerts for critical issues

---

## Module 5: Email Communication System üìß

### Objectives
Implement automated email communications for all user interactions, subscription management, and marketing activities.

### Email Templates Required

#### üîê Authentication Emails
- **Welcome Email**: Brand introduction, getting started guide
- **Email Verification**: Account activation with secure links  
- **OTP Verification**: Time-sensitive verification codes
- **Password Reset**: Secure password reset workflow
- **Login Alerts**: Security notifications for new device logins

#### üí≥ Subscription Emails
- **Plan Selection Confirmation**: Chosen plan details, payment instructions
- **Payment Instructions**: Detailed steps for each payment method
- **Payment Received**: Confirmation of successful payment verification
- **Subscription Activated**: Welcome to paid plan, feature overview
- **Subscription Renewal**: Upcoming renewal reminders
- **Plan Upgrade/Downgrade**: Changes confirmation and effective dates
- **Usage Warnings**: Approaching limits notifications (80%, 95%, 100%)
- **Invoice Generation**: Professional invoices with payment details

#### üìä Scraping Notifications  
- **Scraping Started**: Confirmation of initiated scraping sessions
- **Scraping Completed**: Success summary with data statistics
- **Scraping Failed**: Error details and suggested solutions
- **Export Ready**: Download links for completed exports
- **Scheduled Scraping Reminders**: Upcoming scheduled tasks

#### üõ†Ô∏è Custom Site Requests
- **Request Submitted**: Acknowledgment of custom site request
- **Request Approved**: Development timeline and expectations
- **Development Progress**: Updates on custom scraper development  
- **Scraper Ready**: Custom scraper completed and tested
- **Request Rejected**: Explanation and alternative suggestions

#### üì¢ Marketing & Updates
- **Newsletter**: Platform updates, new features, industry insights
- **Feature Announcements**: New capabilities and improvements
- **Special Offers**: Discount codes, limited-time promotions
- **Webinar Invitations**: Educational content and platform training
- **Survey Requests**: User feedback and platform improvement input

### Expected Output
- Professional email communications that build trust and engagement
- Automated workflows reducing manual support overhead
- Branded email templates consistent with platform design
- Personalized content based on user subscription and usage

### Implementation Approach
- Use Django's built-in email framework with HTML templates
- Implement email queue system for reliable delivery
- Create email template inheritance for consistent branding
- Add email tracking for delivery confirmation and engagement metrics
- Integrate with email service provider (SendGrid, Mailgun) for reliability

---

## Module 6: Landing Page & Marketing Site üåê

### Objectives
Create a professional marketing website that effectively communicates value proposition, showcases features, and converts visitors to customers.

### Page Structure

#### üè† Homepage
- **Hero Section**: Compelling headline, value proposition, CTA button
- **Feature Highlights**: Top 3-4 platform capabilities with icons
- **Pricing Table**: All subscription plans with feature comparison
- **Social Proof**: Customer testimonials, success stories
- **FAQ Section**: Common questions about scraping, pricing, support
- **Trust Signals**: Security badges, uptime guarantees, data protection

#### üí∞ Pricing Page
- **Detailed Plan Comparison**: Feature matrix with clear differences  
- **ROI Calculator**: Interactive tool showing potential savings
- **Payment Methods**: Visual guide to supported payment options
- **Custom Enterprise**: Contact form for large-scale requirements
- **Money-back Guarantee**: Risk-free trial information

#### üõ†Ô∏è Features Page
- **Shopify Integration**: Automated store scraping capabilities
- **Custom Site Support**: Team-developed scraper process
- **Export Options**: CSV, Excel, Google Sheets with examples
- **Scheduling**: Automated scraping with flexible timing
- **Data Quality**: Accuracy guarantees and error handling
- **API Access**: Developer-friendly integration options

#### üìû Contact Page
- **Support Channels**: Email, live chat, support hours
- **Custom Development**: Request form for complex requirements  
- **Partnership Opportunities**: Reseller and affiliate programs
- **Office Information**: Company details and location
- **Response Time**: Expected response times for different inquiries

### Expected Output
- Professional marketing site that builds credibility and trust
- Clear value communication that addresses customer pain points  
- Conversion-optimized design with strong calls-to-action
- SEO-friendly structure for organic traffic growth

### Implementation Approach
- Create separate marketing templates from dashboard
- Implement landing page A/B testing capabilities
- Add conversion tracking and analytics integration
- Optimize for Core Web Vitals and search engine visibility
- Include schema markup for rich search results

---

## Module 7: Manual Payment Processing System üí∞

### Objectives
Implement a secure, efficient system for handling manual payment verification while maintaining user experience and fraud prevention.

### Payment Workflow

#### 1. Plan Selection & Invoice Generation
- User selects subscription plan
- System generates unique invoice with:
  - Invoice number and date
  - Plan details and pricing  
  - Payment instructions for all methods
  - QR codes for crypto payments
  - Deadline for payment completion

#### 2. Payment Method Instructions

**PayPal Transfer**
- Recipient email address
- Payment reference format
- Screenshots required for verification
- Processing time: 24-48 hours

**Fiverr Order**  
- Custom gig creation process
- Order completion workflow
- Automatic verification integration
- Processing time: Instant upon completion

**Payoneer Transfer**
- Account details for transfer
- Reference number requirements  
- Transfer confirmation needed
- Processing time: 48-72 hours

**Cryptocurrency**
- Wallet addresses for USDT, Bitcoin, Ethereum
- QR codes for mobile wallet scanning
- Transaction hash submission required
- Processing time: 2-6 hours after confirmations

#### 3. Payment Verification Process
- User uploads payment proof (screenshots, transaction IDs)
- Admin receives notification for verification
- Verification checklist for each payment method
- Approval/rejection with automatic user notification
- Subscription activation upon approval

#### 4. Fraud Prevention
- IP tracking for suspicious activity
- Payment amount verification
- Timeline validation (payments must be recent)
- Duplicate transaction detection
- User verification for large payments

### Expected Output
- Streamlined payment process despite manual verification
- Secure handling of payment proofs and sensitive data
- Efficient admin workflow for payment verification  
- Clear communication with users throughout process

### Implementation Approach
- Create payment models to track all transactions
- Build secure file upload for payment proofs
- Implement admin verification interface with approval workflow
- Add automated email notifications for all payment stages
- Create audit trail for compliance and dispute resolution

---

## Module 8: Automated Shopify Scraping System üõí

### Objectives
Transform the current static function approach into a dynamic, database-driven system where users can easily add and manage Shopify stores for scraping.

### Current vs. New Approach

#### Current System
- Hardcoded scraper functions for specific stores
- Manual developer intervention for new sites
- Static website configuration
- Limited scalability

#### New Dynamic System  
- User-driven website addition
- Automatic Shopify detection and validation
- Database-stored configuration
- Self-service store management

### Requirements

#### Shopify Store Addition
- **URL Validation**: Verify Shopify store accessibility and structure
- **Store Detection**: Automatic identification of Shopify platform
- **Products API Testing**: Validate JSON API availability
- **Rate Limit Detection**: Identify store's rate limiting policies
- **Configuration Generation**: Automatic scraper settings creation

#### Store Management Interface
- **Add Store**: Simple form with URL input and validation
- **Store List**: User's stores with status indicators
- **Store Settings**: Scraping frequency, specific collections, filters
- **Store Analytics**: Success rates, products found, last scrape data
- **Store Actions**: Edit, pause, resume, delete functionality

#### Dynamic Scraper Configuration
```python
# Database model structure
class ShopifyStore(models.Model):
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    store_url = models.URLField()
    store_name = models.CharField(max_length=200)
    is_active = models.BooleanField(default=True)
    
    # Scraping configuration
    collections_filter = models.JSONField(default=list)  # Specific collections
    products_limit = models.IntegerField(default=250)    # Products per page
    rate_limit_delay = models.IntegerField(default=2)    # Delay between requests
    
    # Validation and status
    last_validated = models.DateTimeField()
    validation_status = models.CharField(max_length=50)
    api_accessible = models.BooleanField(default=True)
    
    # Usage tracking
    scrapes_this_month = models.IntegerField(default=0)
    total_products_found = models.IntegerField(default=0)
    last_successful_scrape = models.DateTimeField(null=True)
```

#### Automatic Store Validation
- **API Accessibility**: Test `/products.json` endpoint
- **Response Structure**: Validate expected JSON format
- **Rate Limiting**: Detect and adapt to store's limits
- **Error Handling**: Graceful handling of blocked or protected stores
- **Store Changes**: Monitor for store modifications affecting scraping

### Expected Output
- Self-service Shopify store addition without developer intervention
- Scalable system handling hundreds of stores per user
- Automatic adaptation to different store configurations
- Improved user experience with instant store addition

### Implementation Approach
- Create dynamic store validation system
- Implement generic Shopify scraper that adapts to store configuration  
- Build user interface for store management
- Add automatic monitoring for store accessibility changes
- Create usage tracking integrated with subscription limits

---

## Module 9: Advanced Scraping Features ‚ö°

### Objectives
Enhance the scraping system with comprehensive scheduling, improved data storage, and advanced session management capabilities.

### Scheduling System

#### Schedule Types
- **One-time**: Immediate scraping execution
- **Recurring**: Daily, weekly, monthly intervals  
- **Custom**: Cron-like expressions for complex schedules
- **Smart Scheduling**: Automatic optimal timing based on store activity

#### Scheduling Interface
- **Calendar View**: Visual schedule management
- **Timezone Support**: User's local timezone handling
- **Conflict Detection**: Prevent overlapping scraping sessions
- **Load Balancing**: Distribute scraping across time slots

#### Advanced Configuration
```python
class ScrapingSchedule(models.Model):
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    website = models.ForeignKey(ShopifyStore, on_delete=models.CASCADE)
    
    # Schedule configuration
    schedule_type = models.CharField(max_length=50)  # one_time, recurring, cron
    cron_expression = models.CharField(max_length=100, null=True)
    timezone = models.CharField(max_length=50, default='UTC')
    
    # Execution settings
    is_active = models.BooleanField(default=True)
    next_run = models.DateTimeField()
    last_run = models.DateTimeField(null=True)
    run_count = models.IntegerField(default=0)
    
    # Advanced options
    max_products = models.IntegerField(null=True)  # Limit products per run
    specific_collections = models.JSONField(default=list)
    notification_settings = models.JSONField(default=dict)
```

### Enhanced Data Storage

#### Comprehensive Product Data
Instead of limited fields, capture complete product information:

```python
class Product(models.Model):
    # Core identification
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    website = models.ForeignKey(ShopifyStore, on_delete=models.CASCADE)
    shopify_id = models.BigIntegerField()
    variant_id = models.BigIntegerField()
    
    # Basic information
    title = models.TextField()
    handle = models.CharField(max_length=500)
    description = models.TextField()
    vendor = models.CharField(max_length=200)
    product_type = models.CharField(max_length=200)
    
    # Variant details
    variant_title = models.CharField(max_length=500)
    sku = models.CharField(max_length=200)
    barcode = models.CharField(max_length=100)
    
    # Pricing
    price = models.DecimalField(max_digits=10, decimal_places=2)
    compare_at_price = models.DecimalField(max_digits=10, decimal_places=2, null=True)
    cost_per_item = models.DecimalField(max_digits=10, decimal_places=2, null=True)
    
    # Inventory
    inventory_quantity = models.IntegerField()
    inventory_policy = models.CharField(max_length=50)
    inventory_management = models.CharField(max_length=50)
    
    # Physical attributes
    weight = models.DecimalField(max_digits=8, decimal_places=3, null=True)
    weight_unit = models.CharField(max_length=10)
    requires_shipping = models.BooleanField()
    
    # SEO and metadata
    seo_title = models.CharField(max_length=500)
    seo_description = models.TextField()
    tags = models.JSONField(default=list)
    
    # Images
    featured_image = models.URLField()
    all_images = models.JSONField(default=list)  # Array of image URLs
    
    # Timestamps and status
    published_at = models.DateTimeField(null=True)
    created_at_shopify = models.DateTimeField()
    updated_at_shopify = models.DateTimeField()
    scraped_at = models.DateTimeField(auto_now=True)
    
    # Raw data backup
    raw_json = models.JSONField()  # Complete Shopify API response
```

#### Flexible Export System
Allow users to select which fields to export:

```python
class ExportConfiguration(models.Model):
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    name = models.CharField(max_length=200)  # "Competition Analysis", "Inventory Report"
    selected_fields = models.JSONField()  # List of field names to export
    filters = models.JSONField(default=dict)  # Product filtering criteria
    is_default = models.BooleanField(default=False)
```

### Stop/Resume Enhancement

#### Advanced Session Control
- **Graceful Stopping**: Complete current product before stopping
- **Smart Resume**: Resume from exact stopping point
- **Pause Scheduling**: Temporarily pause scheduled scraping
- **Priority Queue**: High-priority resume operations

#### Session Persistence
```python
class ScrapingSession(models.Model):
    # ... existing fields ...
    
    # Enhanced resume data
    resume_metadata = models.JSONField(default=dict)
    checkpoint_data = models.JSONField(default=dict)  # Detailed state information
    can_resume = models.BooleanField(default=True)
    resume_priority = models.IntegerField(default=0)
    
    # Performance tracking
    average_products_per_minute = models.FloatField(default=0)
    estimated_completion_time = models.DateTimeField(null=True)
    success_rate = models.FloatField(default=100.0)
```

### Expected Output
- Professional scheduling system rivaling enterprise scraping tools
- Comprehensive data capture enabling detailed analytics
- Flexible export system meeting diverse user needs  
- Reliable session management with perfect resume capability

### Implementation Approach
- Implement Celery Beat for advanced scheduling
- Create comprehensive data models capturing all Shopify fields
- Build flexible export configuration interface
- Add checkpoint system for reliable session resumption
- Implement performance monitoring and estimation algorithms

---

## Module 10: Dynamic Custom Scraper System üîß

### Objectives
Enable team members to create custom scrapers through database-stored code templates, allowing dynamic execution for non-Shopify websites.

### Code Template System

#### Template Structure
All custom scrapers must follow standardized function signatures:

```python
# Template 1: Get Product Links Function
def get_product_links(base_url, config, **kwargs):
    """
    Extract all product page URLs from a website
    
    Args:
        base_url (str): Website's base URL
        config (dict): Website-specific configuration
        **kwargs: Additional parameters
        
    Returns:
        list: List of product page URLs
    """
    import requests
    from bs4 import BeautifulSoup
    import time
    
    # Custom logic here - varies per website
    # Must return list of URLs
    return product_urls

# Template 2: Extract Product Data Function  
def extract_product_data(soup, product_url, website_name, config, **kwargs):
    """
    Extract product information from BeautifulSoup object
    
    Args:
        soup (BeautifulSoup): Parsed HTML of product page
        product_url (str): URL of the product page
        website_name (str): Name of the website
        config (dict): Website-specific configuration
        **kwargs: Additional parameters
        
    Returns:
        dict: Standardized product data dictionary
    """
    # Custom extraction logic here - varies per website
    # Must return standardized dictionary format
    
    return {
        'name': '',
        'sku': '',
        'price': '',
        'description': '',
        'images': [],
        'in_stock': True,
        'brand': '',
        'category': '',
        'specifications': {},
        'raw_data': {}  # Any additional data
    }
```

#### Database Model
```python
class CustomScraperTemplate(models.Model):
    # Website association
    website_request = models.OneToOneField('CustomWebsiteRequest', on_delete=models.CASCADE)
    
    # Developer information
    created_by = models.ForeignKey(User, on_delete=models.CASCADE)  # Team member
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    # Code templates
    get_links_code = models.TextField(help_text="Python code for get_product_links function")
    extract_data_code = models.TextField(help_text="Python code for extract_product_data function")
    
    # Configuration
    website_config = models.JSONField(default=dict)  # Website-specific settings
    required_headers = models.JSONField(default=dict)  # Custom headers if needed
    rate_limit_delay = models.IntegerField(default=3)  # Seconds between requests
    
    # Testing and validation
    test_urls = models.JSONField(default=list)  # URLs for testing scraper
    last_tested = models.DateTimeField(null=True)
    test_results = models.JSONField(default=dict)  # Testing outcomes
    
    # Status
    is_active = models.BooleanField(default=False)
    status = models.CharField(max_length=50, default='development')  # development, testing, active, deprecated
    
    # Performance tracking
    success_rate = models.FloatField(default=0)
    average_extraction_time = models.FloatField(default=0)
    total_products_scraped = models.IntegerField(default=0)
```

### Custom Website Request System

#### Request Workflow
```python
class CustomWebsiteRequest(models.Model):
    # User request details
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    website_url = models.URLField()
    website_name = models.CharField(max_length=200)
    business_reason = models.TextField()  # Why user needs this website
    
    # Request details
    expected_data_fields = models.JSONField()  # What fields user expects
    scraping_frequency = models.CharField(max_length=50)  # How often they plan to scrape
    estimated_products = models.IntegerField(null=True)  # Expected number of products
    
    # Priority and complexity
    priority = models.CharField(max_length=20, default='normal')  # low, normal, high, urgent
    complexity = models.CharField(max_length=20)  # simple, medium, complex, advanced
    estimated_hours = models.IntegerField(null=True)  # Development time estimate
    
    # Status tracking
    status = models.CharField(max_length=50, default='submitted')
    # submitted -> reviewed -> approved/rejected -> development -> testing -> completed
    
    assigned_developer = models.ForeignKey(User, on_delete=models.SET_NULL, null=True, related_name='assigned_requests')
    
    # Timeline
    submitted_at = models.DateTimeField(auto_now_add=True)
    reviewed_at = models.DateTimeField(null=True)
    approved_at = models.DateTimeField(null=True) 
    development_started_at = models.DateTimeField(null=True)
    completed_at = models.DateTimeField(null=True)
    
    # Communication
    admin_notes = models.TextField(blank=True)  # Internal team notes
    user_feedback = models.TextField(blank=True)  # User's feedback on completed scraper
    rejection_reason = models.TextField(blank=True)  # Why request was rejected
```

### Safe Code Execution Engine

#### Security Framework
```python
class SafeScraperExecutor:
    """Secure execution environment for custom scraper code"""
    
    ALLOWED_IMPORTS = [
        'requests', 'bs4', 're', 'json', 'time', 'datetime',
        'urllib.parse', 'random', 'math', 'string'
    ]
    
    BLOCKED_FUNCTIONS = [
        'exec', 'eval', 'open', '__import__', 'compile',
        'globals', 'locals', 'vars', 'dir', 'getattr', 'setattr'
    ]
    
    def __init__(self, timeout=60):
        self.timeout = timeout
        
    def execute_scraper_function(self, code, function_name, *args, **kwargs):
        """Execute scraper function with security restrictions"""
        
        # Code validation
        self.validate_code_safety(code)
        
        # Create restricted execution environment
        safe_globals = self.create_safe_environment()
        
        # Execute with timeout
        with self.execution_timeout():
            exec(code, safe_globals)
            
        # Get function and execute
        if function_name not in safe_globals:
            raise ValueError(f"Function {function_name} not found")
            
        return safe_globals[function_name](*args, **kwargs)
        
    def validate_code_safety(self, code):
        """Validate code using AST parsing"""
        # Implementation of AST validation
        # Check for forbidden functions, imports, etc.
        pass
        
    def create_safe_environment(self):
        """Create restricted execution environment"""
        # Implementation of safe globals dictionary
        pass
```

### Admin Development Interface

#### Developer Workflow
1. **Request Review**: Admin reviews incoming custom website requests
2. **Complexity Assessment**: Estimate development time and assign priority
3. **Developer Assignment**: Assign request to available team member
4. **Template Creation**: Developer creates code templates using admin interface
5. **Testing Phase**: Test scraper against provided URLs
6. **User Validation**: User tests scraper and provides feedback
7. **Production Deployment**: Activate scraper for user's account

#### Admin Interface Features
- **Code Editor**: Syntax highlighting, error detection, auto-completion
- **Testing Tools**: One-click testing against sample URLs
- **Template Library**: Reusable code snippets for common scenarios
- **Performance Monitoring**: Track scraper efficiency and success rates
- **User Communication**: Direct messaging with requesting user

### Expected Output
- Scalable system for adding custom website support
- Secure code execution without compromising server security
- Efficient workflow for team to develop and deploy scrapers  
- Professional template system ensuring code quality and consistency

### Implementation Approach
- Implement secure Python code execution with AST validation
- Create comprehensive admin interface for code management
- Build testing framework with automated validation
- Add performance monitoring for custom scrapers
- Implement user feedback system for continuous improvement

---

## üöÄ Future Roadmap & Extensions

### Phase 2: Social Media Integration üì±
- **Instagram Profile Scraper**: Extract profile details, follower metrics, post analytics
- **Instagram Contact Extraction**: Email addresses, phone numbers, business information
- **LinkedIn Profile Data**: Professional information, company details, connections
- **Twitter/X Analytics**: Follower growth, engagement metrics, trending hashtags
- **Facebook Business Pages**: Contact information, reviews, business hours

### Phase 3: E-commerce Automation Scripts ü§ñ
- **Amazon Price Tracker**: Monitor competitor pricing, stock levels, review changes
- **eBay Market Analysis**: Auction tracking, sold listings analysis, trending products
- **Shopify Store Analytics**: Competitor store monitoring, app usage tracking, theme analysis
- **Inventory Management**: Multi-platform stock synchronization, low stock alerts
- **Price Intelligence**: Dynamic pricing recommendations based on market data

### Phase 4: API & Integration Platform üîó
- **RESTful API**: Full-featured API for third-party integrations
- **Webhook Support**: Real-time notifications for completed scrapes
- **Zapier Integration**: Connect with 3000+ apps and services
- **Custom Integrations**: Direct database connections, CRM sync, analytics platforms
- **White-label Solutions**: Reseller program with branded dashboards

---

## üìà Implementation Timeline & Milestones

### Phase 1: Foundation (Weeks 1-8)
**Week 1-2: UI/UX Redesign**
- [ ] Design system creation (colors, typography, components)
- [ ] Homepage and dashboard mockups
- [ ] Responsive template development
- [ ] Dark theme implementation

**Week 3-4: Multi-Tenant System**
- [ ] User model extensions and profile management
- [ ] Google OAuth2 integration
- [ ] Email verification system
- [ ] Data isolation middleware

**Week 5-6: Subscription System**
- [ ] Subscription models and plan configuration
- [ ] Usage tracking implementation
- [ ] Payment workflow development
- [ ] Admin verification interface

**Week 7-8: Email & Communication**
- [ ] Email template creation (20+ templates)
- [ ] Automated email workflows
- [ ] Notification system
- [ ] SMTP integration and testing

### Phase 2: Core Features (Weeks 9-16)
**Week 9-10: Super Admin Dashboard**
- [ ] User management interface
- [ ] Subscription analytics
- [ ] System monitoring dashboard
- [ ] Revenue tracking

**Week 11-12: Landing Page & Marketing**
- [ ] Marketing website development
- [ ] SEO optimization
- [ ] Conversion tracking setup
- [ ] Content management system

**Week 13-14: Manual Payment System**
- [ ] Payment method integrations
- [ ] Proof upload system
- [ ] Verification workflow
- [ ] Invoice generation

**Week 15-16: Automated Shopify System**
- [ ] Dynamic store addition interface
- [ ] Store validation system
- [ ] Generic scraper adaptation
- [ ] Usage limit enforcement

### Phase 3: Advanced Features (Weeks 17-24)
**Week 17-18: Advanced Scraping**
- [ ] Scheduling system with calendar UI
- [ ] Enhanced data models
- [ ] Flexible export configuration
- [ ] Session management improvements

**Week 19-20: Dynamic Custom Scrapers**
- [ ] Safe code execution engine
- [ ] Custom scraper templates
- [ ] Admin development interface
- [ ] Testing framework

**Week 21-22: System Integration**
- [ ] End-to-end testing
- [ ] Performance optimization
- [ ] Security audit
- [ ] Bug fixes and refinements

**Week 23-24: Launch Preparation**
- [ ] Production deployment setup
- [ ] Monitoring and alerting
- [ ] Documentation completion
- [ ] User training materials

---

## üõ†Ô∏è Technology Stack Recommendations

### Backend Technologies
- **Framework**: Django 5.2+ with Django REST Framework
- **Database**: PostgreSQL 15+ with Redis for caching
- **Task Queue**: Celery with Redis broker
- **Authentication**: Django-allauth + Google OAuth2
- **File Storage**: AWS S3 or Google Cloud Storage
- **Email Service**: SendGrid or Amazon SES

### Frontend Technologies
- **CSS Framework**: Bootstrap 5 with custom SCSS variables
- **JavaScript**: Vanilla JS with minimal jQuery for interactions
- **Charts & Analytics**: Chart.js or ApexCharts
- **Calendar**: FullCalendar.js for scheduling interface
- **Rich Text Editor**: TinyMCE for email template editing

### DevOps & Infrastructure
- **Containerization**: Docker with Docker Compose
- **Web Server**: Nginx with Gunicorn
- **SSL**: Let's Encrypt with automatic renewal
- **Monitoring**: Prometheus + Grafana or New Relic
- **Log Management**: ELK Stack (Elasticsearch, Logstash, Kibana)
- **Backup**: Automated database backups with rotation

### Development Tools
- **Version Control**: Git with feature branch workflow
- **CI/CD**: GitHub Actions or GitLab CI
- **Code Quality**: Black (formatting) + Flake8 (linting) + pytest
- **Documentation**: Sphinx for API docs, MkDocs for user guides
- **Security Scanning**: Bandit for Python security issues

---

## üóÑÔ∏è Database Schema Changes

### New Core Models
```python
# User Management
class UserProfile(models.Model)
class UserSubscription(models.Model)
class UsageTracking(models.Model)

# Payment System  
class PaymentMethod(models.Model)
class PaymentTransaction(models.Model)
class Invoice(models.Model)

# Shopify System
class ShopifyStore(models.Model)
class ScrapingSchedule(models.Model)
class ExportConfiguration(models.Model)

# Custom Scrapers
class CustomWebsiteRequest(models.Model)
class CustomScraperTemplate(models.Model)
class ScraperTestResult(models.Model)

# System Management
class SystemConfiguration(models.Model)
class AdminNotification(models.Model)
class AuditLog(models.Model)
```

### Enhanced Existing Models
```python
# Add user relationships and enhanced fields
class Product(models.Model):
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    # ... comprehensive Shopify fields
    raw_json = models.JSONField()  # Complete API response

class ScrapingSession(models.Model):
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    # ... enhanced session tracking
    performance_metrics = models.JSONField(default=dict)
```

### Migration Strategy
1. **Phase 1**: Add user relationships to existing models
2. **Phase 2**: Create subscription and payment models
3. **Phase 3**: Implement custom scraper models
4. **Phase 4**: Add comprehensive product data fields
5. **Phase 5**: System configuration and audit models

---

## üîí Security Considerations

### Authentication & Authorization
- **Multi-Factor Authentication**: SMS/Email OTP for sensitive operations
- **Session Security**: Secure session cookies with timeout
- **Password Policy**: Strong password requirements with complexity rules
- **Rate Limiting**: API rate limiting to prevent abuse
- **CSRF Protection**: Built-in Django CSRF protection

### Data Protection
- **Data Encryption**: Encrypt sensitive data at rest (payment info, personal details)
- **HTTPS Enforcement**: SSL/TLS for all communications
- **Input Validation**: Comprehensive input sanitization and validation
- **SQL Injection Prevention**: Use Django ORM exclusively
- **XSS Protection**: Template auto-escaping and CSP headers

### Custom Code Execution Security
- **Sandboxed Environment**: Restricted Python execution environment
- **AST Validation**: Code analysis before execution
- **Resource Limits**: CPU and memory restrictions
- **Network Isolation**: Limited network access for custom code
- **Audit Trail**: Log all custom code executions

### Infrastructure Security
- **Firewall Configuration**: Restrict access to necessary ports only
- **Database Security**: Encrypted connections, limited user privileges
- **Backup Encryption**: Encrypted database backups
- **Monitoring**: Real-time security monitoring and alerting
- **Compliance**: GDPR compliance for EU users

---

## üöÄ Deployment & DevOps

### Production Environment
```yaml
# Docker Compose structure
services:
  web:
    image: scrapemasterpro/web:latest
    environment:
      - DJANGO_SETTINGS_MODULE=core.settings.production
    
  database:
    image: postgres:15
    environment:
      - POSTGRES_DB=scrapemasterpro
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    
  redis:
    image: redis:7-alpine
    
  celery:
    image: scrapemasterpro/web:latest
    command: celery -A core worker -l info
    
  celery-beat:
    image: scrapemasterpro/web:latest
    command: celery -A core beat -l info
```

### Environment Configuration
- **Development**: Local development with Docker Compose
- **Staging**: Production-like environment for testing
- **Production**: Multi-server setup with load balancing
- **Backup**: Automated daily backups with offsite storage

### Monitoring & Alerting
- **Application Performance**: Response times, error rates, throughput
- **System Resources**: CPU, memory, disk usage, network traffic
- **Business Metrics**: User registrations, subscription conversions, scraping success rates
- **Error Tracking**: Automatic error reporting with stack traces
- **Uptime Monitoring**: External monitoring for availability

---

## üß™ Testing Strategy

### Test Coverage Requirements
- **Unit Tests**: 90%+ code coverage for core business logic
- **Integration Tests**: API endpoint testing, database operations
- **End-to-End Tests**: Complete user workflows (registration to scraping)
- **Performance Tests**: Load testing for scraping operations
- **Security Tests**: Penetration testing, vulnerability scanning

### Testing Framework
```python
# Test structure
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îú‚îÄ‚îÄ test_views.py
‚îÇ   ‚îî‚îÄ‚îÄ test_scrapers.py
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py
‚îÇ   ‚îú‚îÄ‚îÄ test_payment_flow.py
‚îÇ   ‚îî‚îÄ‚îÄ test_scraping_workflow.py
‚îú‚îÄ‚îÄ e2e/
‚îÇ   ‚îú‚îÄ‚îÄ test_user_registration.py
‚îÇ   ‚îú‚îÄ‚îÄ test_shopify_scraping.py
‚îÇ   ‚îî‚îÄ‚îÄ test_export_functionality.py
‚îî‚îÄ‚îÄ performance/
    ‚îú‚îÄ‚îÄ test_load_scraping.py
    ‚îî‚îÄ‚îÄ test_concurrent_users.py
```

### Quality Assurance
- **Code Review**: All code changes require peer review
- **Automated Testing**: CI/CD pipeline runs full test suite
- **Manual Testing**: QA team validates new features
- **User Acceptance Testing**: Beta testing with select users
- **Bug Tracking**: Comprehensive issue tracking and resolution

---

## üìö Documentation Requirements

### Technical Documentation
- **API Documentation**: Complete REST API reference with examples
- **Database Schema**: Entity-relationship diagrams and field descriptions
- **Deployment Guide**: Step-by-step production deployment instructions
- **Development Setup**: Local development environment setup
- **Architecture Overview**: System design and component interactions

### User Documentation
- **Getting Started Guide**: New user onboarding and first scrape
- **Feature Tutorials**: Step-by-step guides for all major features
- **FAQ**: Common questions and troubleshooting
- **Video Tutorials**: Screen-recorded feature demonstrations
- **Best Practices**: Recommendations for optimal scraping strategies

### Admin Documentation
- **Admin Panel Guide**: Complete administrator interface documentation
- **Custom Scraper Development**: Template creation and testing procedures
- **Payment Verification**: Manual payment processing workflows
- **System Monitoring**: Monitoring dashboard usage and alerting setup
- **Troubleshooting**: Common issues and resolution procedures

---

## üí∞ Business Model & Pricing Strategy

### Revenue Streams
1. **Subscription Revenue**: Monthly recurring revenue from plans
2. **Add-on Sales**: Extra sites and scraping hits
3. **Custom Development**: Premium custom scraper development
4. **Enterprise Licenses**: White-label and API access
5. **Training & Consulting**: Professional services for large clients

### Competitive Analysis
- **Direct Competitors**: Scrapy Cloud, Apify, Octoparse
- **Pricing Comparison**: Position as mid-market solution
- **Unique Value Proposition**: Manual payment flexibility for global market
- **Market Positioning**: Focus on e-commerce businesses and researchers

### Growth Strategy
- **Free Tier**: Generous free plan for user acquisition
- **Content Marketing**: SEO-optimized blog content about web scraping
- **Partner Program**: Affiliate and reseller partnerships
- **API Ecosystem**: Third-party integrations and marketplace presence
- **Community Building**: User forums and knowledge sharing

---

## üìä Success Metrics & KPIs

### Business Metrics
- **Monthly Recurring Revenue (MRR)**: Target growth rate
- **Customer Acquisition Cost (CAC)**: Marketing efficiency
- **Customer Lifetime Value (CLV)**: Long-term profitability
- **Churn Rate**: Customer retention measurement
- **Conversion Rate**: Free to paid plan conversions

### Technical Metrics
- **System Uptime**: 99.9% availability target
- **Scraping Success Rate**: Data extraction reliability
- **API Response Time**: Performance benchmarks
- **Error Rate**: System stability measurement
- **User Satisfaction**: Support ticket resolution time

### Product Metrics
- **Feature Adoption**: Usage of new features
- **User Engagement**: Active users and session length
- **Export Volume**: Data export frequency and size
- **Custom Scraper Requests**: Demand for custom development
- **Support Ticket Volume**: Product usability indicator

---

## üéØ Project Summary

**ScrapeMaster Pro** represents a complete transformation of your existing Django scraper into a world-class SaaS platform. This comprehensive solution addresses the global market need for accessible web scraping tools while overcoming payment gateway limitations through innovative manual verification processes.

The platform combines automated Shopify scraping with custom website support, creating a unique value proposition that scales from individual researchers to enterprise clients. With robust subscription management, advanced scheduling capabilities, and a secure custom scraper development system, ScrapeMaster Pro is positioned to capture significant market share in the growing web scraping industry.

The modular architecture ensures each component can be developed and deployed independently, allowing for iterative improvement and rapid feature rollout. The emphasis on security, scalability, and user experience creates a solid foundation for long-term growth and expansion into adjacent markets like social media scraping and e-commerce automation.

This requirements document serves as the definitive guide for transforming your vision into a market-leading SaaS platform that empowers businesses worldwide to harness the power of web data.
